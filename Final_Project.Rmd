---
title: "Super PACs in the 2018 Election Year"
author: "Bridget Silha"
date: "12/14/2019"
header-includes:
   - \usepackage{setspace}
   - \doublespacing
output:
  pdf_document:
    latex_engine: xelatex
fontsize: 12pt
---

```{r, include = FALSE}
require(tidyverse)
require(caret) # for machine learning
require(recipes) # For preprocessing your data
require(rattle) # For nice tree plots
require(lubridate) # For rounding dates

doMC::registerDoMC()
```

# Introduction

The aim of this project is to analyze and predict the factors necessary for producing an Independent Expenditure-Only Committee (Super PAC). There are several goals of this project including:

- Assess the scale and magnitude of the effect that Citizens United v. FEC had on FEC filings and registration
- Discuss the financial statements of Super PACs compared to other political committees during the 2018 Election Cycle
- Using supervised learning, judge whether or not classification algorithms can be respectfully used in order to predict whether a political committee is a Super PAC or not

For this project, I will be using data from the Federal Election Commission (FEC) website from the 2018 Election Cycle. First, I will join together political action committee (PAC) information with their financial statements from that election cycle. After cleaning that dataset, I will then analyze several variables in-depth (including registered state, disbursments, contributions, receipts, and filing time). Finally, I will create several models that attempt to predict with respectable accuracy if a political committee is a Super PAC or not. 

# Problem Statement and Background

The Federal Election Commisssion (FEC) is an independent, governmental agency that is responsible for "administering and enforcing campaign finance law".[^1] They are also responsible for financial disclosure statements to/from candidates and committees that influence public office. These records are public information. Many committees, such as PACs and individuals, are subject to contribution limits. However, Independent Expenditure-Only Committees (Super PACs) are not. 

Super PACs were initiated in January 2010 after *Citizens United v. FEC* was decided. The Supreme Court in this case maintained that freedom of speech was essential to democracy and that speech from a corporation was no exception. This new kind of committee would be able to bypass fundraising and expenditure limitations set on other political committees. The only limitation Super PACs have is that they may not coordinate with candidates in their express advocacy for/against them. Since their birth early this decade, Super PACs have collectively spent over $2 billion.[^2] 

[^1]: https://www.fec.gov/about/mission-and-history/
[^2]: https://fas.org/sgp/crs/misc/R42042.pdf 

# Data

The two datasets that I patched together were taken from the FEC's regularly maintained website. The unit of observation is *committee id*. 
Below are the steps that I did in order to wrangle the data: 

1. Read in CSVs from FEC website

```{r, include=FALSE}
## -------------------------------------- Data: Reading in Data --------------------------------------------##

#Read CVS data in
total_fec_committee_party_and_pac <- read_csv("committees.csv")
##DATA DESCRIPTION FOR COMMITTEE TYPE: https://www.fec.gov/campaign-finance-data/committee-type-code-descriptions/ 

pac_financial_info_2018 <- read_csv("committee_summary_2018.csv")
##DATA DESCRIPTION: https://www.fec.gov/campaign-finance-data/committee-summary-file-description/ 
```

2. Joined the two CSVs based on *committee id*

```{r, include = FALSE}
## ----------------------------------- Data: Combining CVS ---------------------------------##

#Bind 2018 committee information and financial information together
new_data <- right_join(total_fec_committee_party_and_pac, pac_financial_info_2018, by=c("committee_id" = "CMTE_ID"), c("name" = "CMTE_NM"))

```

3. Cut down unnecessary variables in the dataset (such as committee treasurer name and committee zipcode)

```{r, include = FALSE}
## ----------------------------------- Data: Cut Down Unnecesary Variables ---------------------------------##

#Cut down variables
new_data <- new_data %>% 
  select (-Link_Image, -CMTE_ST1:-CMTE_CITY, -CMTE_ZIP:-CAND_ID, -INDV_CONTB:-OTH_CMTE_CONTB, -TRANF_FROM_OTHER_AUTH_CMTE:-OTHER_RECEIPTS, -TRANF_TO_OTHER_AUTH_CMTE:-OTHER_DISB, -NET_CONTB:-TTL_COMMUNICATION_COST, -COH_BOY, -COH_COY, -name, -cycles, -treasurer_name, -filing_frequency:-affiliated_committee_name, -party_full, -candidate_ids, -FEC_ELECTION_YR, -CMTE_TP)

#Committee name and commmitte id as first variables
new_data <- new_data %>% 
  select (CMTE_NM, committee_id, everything())
```

4. Filtered out results to only accept PAC committees based on the variable *committee_type*

```{r, include = FALSE}
## ------------- Data: Filter for PACs ------------------------------##

#Filter for committee type -- only PACs
new_data <- new_data %>% 
  filter(committee_type == "N" | committee_type == "O" | committee_type == 
           "Q" | committee_type == "V" | committee_type == "W")
```

5. Filtered out committees that had been terminated in the past based on the variable *CMTE_FILING_FREQ*

```{r, include = FALSE}
## --------------- Data: Filter for Active Committees ----------------##

#Filter out terminated committees
new_data <- new_data %>% 
  filter(CMTE_FILING_FREQ == "M" | CMTE_FILING_FREQ == "Q")
```


6. Made *first_filing_date*, *last_f1_date*, and *last_file_date* variables into a "Date" class


```{r, include = FALSE}
## ----------------------------------- Data: Changing variables to Date using lubridate ---------------------------------##

# Formatting original columns as MM-DD-YY format
new_data$first_file_date <- as.Date(new_data$first_file_date, format = "%m/%d/%y")
new_data$last_f1_date <- as.Date(new_data$last_f1_date, format = "%m/%d/%y")
new_data$last_file_date <- as.Date(new_data$last_file_date, format = "%m/%d/%y")

# Round to nearest month
new_data$first_file_month <- round_date(new_data$first_file_date, unit = "month"); new_data$first_file_month
new_data$last_f1_date_month <- round_date(new_data$last_f1_date, "month")
new_data$last_file_date_month <- round_date(new_data$last_file_date, "month")

# Name the variables the month name
new_data$first_file_month <- as.Date(new_data$first_file_month, format = "%B"); new_data$first_file_month
new_data$last_f1_date_month <- as.Date(new_data$last_f1_date_month, format = "%B")
new_data$last_file_date_month <- as.Date(new_data$last_file_date_month, format = "%B")

# Round to nearest year
new_data$first_file_year <- round_date(new_data$first_file_date, unit = "year")
new_data$last_f1_date_year <- round_date(new_data$last_f1_date, "year")
new_data$last_file_date_year <- round_date(new_data$last_file_date, "year")

# Name the variables the year name
new_data$first_file_year <- as.Date(new_data$first_file_year, format = "%Y")
new_data$last_f1_date_year <- as.Date(new_data$last_f1_date_year, format = "%Y")
new_data$last_file_date_year <- as.Date(new_data$last_file_date_year, format = "%Y")


str(new_data)
```


7. Created a new variable for the dataset, *SuperPAC_yes*, which identifies whether or not the specific observation is a SuperPAC based on *committee_type*


```{r, include = FALSE}
## ----------------------------------- Cleaning data: Adding new Super PAC_yes response variable ---------------------------------##

# Add new variable to dataset
# if , Then _yes = 1

new_data <- new_data %>% 
  mutate(SuperPAC_yes = recode(committee_type, "O" = 1, "N" = 0, "V" = 0, "W" = 0, "Q" = 0, .default = 0))

```

Below you can see the variables of interest:
```{r}
new_data %>% colnames()
```

# Analysis 

## Split the Data

First, I split the *new_data* dataset into two further datasets based on our response variables, *SuperPAC_yes*. These datasets, the training and test dataset, are essential to verify that any model or predictions that are made can be applied to "unseen" data. By doing this, we can also reduce the error for "overfitting" the model to our training data. For this analysis, the training and test data will share an 80-20 split of the response variable values. 

```{r, include = FALSE}
## --------------------- Splitting data into training and test datasets -------------------##

#split into training and test data
index = createDataPartition(new_data$SuperPAC_yes,p=.8,list=F) 
train_data = new_data[index,] # Use 80% of the data as training data 
test_data = new_data[-index,] # holdout 20% as test data 

# Proporation in the training data 
round(nrow(train_data)/nrow(new_data),3)

# Proporation in the test data 
round(nrow(test_data)/nrow(new_data),3)

#number of observations in training data
dim(train_data)
dim(test_data)

```

## Understanding the Outcome

Our response variable, *SuperPAC_yes*, has been manually created based off of another variables, *committee_type*. It takes on the value of "1" if the committee is a Super PAC and "0" if it is any other type of PAC. In the training dataset, there seems to be roughly a 1:4 ratio between Super PAC and other PAC types. There are 4,967 observations in the training dataset. 


```{r, fig.width=3, fig.height=3, echo = FALSE}

train_data %>% 
  ggplot(aes(SuperPAC_yes)) +
  geom_bar()

train_data %>% nrow()
```

## Understanding the Features

### Conlusions

Looking at the categorical variables in the dataset, there seems to be good variation in variables such as *ORG_TP* and *CMTE_FILING_FREQ*. In the variable *committee_type*, there seems to be over-represetnation from PAC types like "PAC - Qualified" and "PAC - Nonqualified" (shown as "Q" and "N" below) compared to PAC with Non-contribution Account - Nonqualified" and "PAC with Non-contribution Account - Nonqualified" (shown as "W" and "V" below). 


```{r, fig.width=7, fig.height=3, echo = FALSE}
## --------------------- Data Visualization: categorical variables -------------------##

#Visualize distribution of categorical variables
train_data %>% 
  select(CMTE_DSGN, CMTE_FILING_FREQ, committee_type, ORG_TP) %>% 
  gather(var,val) %>% 
  ggplot(aes(val)) +
  geom_bar() +
  scale_y_log10() +
  facet_wrap(~var,scales="free",ncol=3)
```

When looking at the date variables within the training dataset, there are several trends that can be observed. For example, when plotting *last_file_date* frequencies, we can see that the most frequent filing date months for the 2018 election year was in July and October. This is consistent with the information that we found from *CMTE_FILING_FREQ* in which most committees file on a quarterly basis (Q3 being in October) while only some file on a semi-annual basis (semi-annual deadline being in July). 


```{r, include = FALSE}
## ------------------------- Data Visualization: date variables ---------------------##

#Visualize distribution of date variables

train_data %>% 
ggplot(aes(x=last_file_date)) + 
  geom_histogram(binwidth=5, colour="white") +
       ylab("Frequency") + 
  xlab("Year and Month") +
       theme_bw()

train_data %>% 
ggplot(aes(x=first_file_date)) + 
  geom_histogram(binwidth=100, colour="white") +
       ylab("Frequency") + 
  xlab("Year") +
       theme_bw()

train_data %>% 
ggplot(aes(x=last_f1_date)) + 
  geom_histogram(binwidth=50, colour="white") +
       ylab("Frequency") + 
  xlab("Year") +
       theme_bw()


```


## Understanding the Relationship Between Features

### Conclusions:

As seen below, many of the "power states" that are usually the center of elections (i.e. VA, DC, NY, CA) are the state that most PACs are registered in. However, there is minimal evidence to support the fact that these states produce more Super PACs as compared to other states (as seen in the figure below). Most of the registration in these states are composed of the PAC type "PAC - Qualified". However, looking at a temporal analysis of the *first_filing_date* variables, there has been an uptick in "PAC - Nonqualified" and "Super PAC" registration in the last decade. This rise in Super PAC registration is almost certainly linked to the *Citizens United v. FEC* Supreme Court case (seen in the figure as the black, vertical line). which ruled that these PACs were legal and distinct from other PACs when registering. However, one can note that the rise of "PAC - Nonqualified" committee types has surpassed Super PACs in registration even before *Citizens United* was decided. 

```{r, fig.width=6, fig.height=4, echo = FALSE, warning = FALSE, message = FALSE}
## ------------------------- Data Visualization: More in-depth ---------------------##

# Load
require ("RColorBrewer")

#Coverage over states vs. initial filing date grouped by committee type
new_data %>% 
  ggplot(aes(first_file_date, CMTE_ST, color = factor(committee_type))) +
  geom_point(show.legend = T) +
  geom_vline(xintercept = as.numeric(as.Date("2010-01-21"))) +
  #Line to show when Citizens United became official
  scale_fill_discrete(name="PAC Type",
                         breaks=c("O", "N", "Q", "V", "W"),
                         labels = c("Independent Expenditure-Only (Super PACs)", "PAC - Nonqualified", "PAC - Qualified", "PAC with Non-contribution Account - Nonqualified", "PAC with Non-contribution Account - Qualified")) +
  scale_colour_manual(name="PAC Type",
                      breaks=c("N", "O", "Q", "V", "W"), 
                      values = c((brewer.pal(5,"Set1"))),
                      labels = c("Independent Expenditure-Only (Super PACs)", "PAC - Nonqualified", "PAC - Qualified", "PAC with Non-contribution Account - Nonqualified", "PAC with Non-contribution Account - Qualified")) +
  ggtitle("PAC Filing Over Time by State and PAC Type") +
  xlab ("First File Date") +
  ylab ("State of Registration")

```


```{r, include = FALSE}
## -------------------------------- Data Visualization: Round Dates ----------------------------##
#Round date variables to nearest month/year
train_data1 <- train_data %>% 
  mutate (first_file_date = round_date(first_file_date, unit = "month")) %>% 
  mutate (last_f1_date = round_date(last_f1_date, unit = "month")) %>% 
  mutate (last_file_date = round_date(last_file_date, unit = "month"))
test_data1 <- test_data %>% 
  mutate (first_file_date = round_date(first_file_date, unit = "month")) %>% 
  mutate (last_f1_date = round_date(last_f1_date, unit = "month")) %>% 
  mutate (last_file_date = round_date(last_file_date, unit = "month"))
```

```{r, fig.width=7, fig.height=3, echo = FALSE}

#PAC Filing over Time by PAC Type
train_data1 %>% 
  ggplot(aes(first_file_date, color = committee_type)) +
  geom_bar() +
  stat_count(width = 10) +
  scale_fill_discrete(name="PAC Type",
                         breaks=c("N", "O", "Q", "V", "W"),
                         labels = c("PAC - Nonqualified", "Independent Expenditure-Only (Super PACs)", "PAC - Qualified", "PAC with Non-contribution Account - Nonqualified", "PAC with Non-contribution Account - Qualified")) +
  scale_colour_manual(name="PAC Type",
                      breaks=c("N", "O", "Q", "V", "W"), 
                      values = c((brewer.pal(5,"Set1"))),
                      labels = c("PAC - Nonqualified", "Independent Expenditure-Only (Super PACs)" , "PAC - Qualified", "PAC with Non-contribution Account - Nonqualified", "PAC with Non-contribution Account - Qualified")) +
  xlab("Date of First Filing with FEC") +
  ylab("Number of PACs") +
  ggtitle("PAC Filing Over Time by PAC Type") +
  geom_vline(xintercept = as.numeric(as.Date("2010-01-21"))) +
  ggthemes::theme_tufte()

```

## Unpacking the Outcome

### Pre-process the data

In order to have an accurate model, pre-processing both the test and training dataset is necessary. This includes three steps:

1. Scaling those variables that are not within the same range as other variables
2. Creating new "dummy" variables to substitute for categorical variables
3. Detecting missing values and imputing

The first step in my pre-processing method was to take the log() function of all variables that dealt with large sums of money. For the training dataset, that meant taking the natural log of *TTL_CONTB*, *TTL_RECEIPTS*, and *TTL_DISB*. Through this process, it was easier to visualize the differences between these vairables when it came to different committee types and in which state they were registered. In the figure below, it's obvious that PAC-Qualified is the committee type that most often spends and receives the most in each state. Also, most states are pretty similar in the ranges of their contributions, receipts, and expenditures. However, there are states were the ranges of these values are larger for each variables (i.e. DC, NY, CA, etc). There is also a much larger range for total contributions than total disbursements or receipts in most states. This makes sense since contributions are only one aspect of receipts. Contributions are defined as something of monetary value, whereas receipts can be monetary or non-monetary value. For example, volunteering hours to canvas for a campaign. 

```{r, include = FALSE}
## -------------------------------- Pre-Process the Data: Log money variables ----------------------------##
#Convert TTL_CONTB, TTL_RECEIPTS, TTL_DISB to log

convert_TTL_CONTB <- . %>% mutate(TTL_CONTB = log(TTL_CONTB+1))
convert_TTL_RECEIPTS <- . %>% mutate(TTL_RECEIPTS = log(TTL_RECEIPTS+1))
convert_TTL_DISB <- . %>% mutate(TTL_DISB = log(TTL_DISB+1))

#apply function to both training and test databases

train_data2 <- train_data1 %>%
  convert_TTL_CONTB() %>% 
  convert_TTL_RECEIPTS() %>% 
  convert_TTL_DISB()

test_data2 <- test_data1 %>% 
  convert_TTL_CONTB() %>% 
  convert_TTL_RECEIPTS() %>% 
  convert_TTL_DISB()

#Visualize
train_data2 %>% 
  ggplot(aes(TTL_CONTB)) +
  geom_histogram(binwidth = 1)

train_data2 %>% 
  ggplot(aes(TTL_DISB)) +
  geom_histogram(binwidth = 1)

train_data2 %>% 
  ggplot(aes(TTL_RECEIPTS)) +
  geom_histogram(binwidth = 1)


```

```{r, include = FALSE}
## -------------------------------- Pre-Process the Data: Visualize log(money) variables ----------------------------##
#Mapped STATE vs. contributions
contb <- train_data2 %>% 
  ggplot(aes(TTL_CONTB, CMTE_ST, color = factor(committee_type))) +
    scale_fill_discrete(name="PAC Type",
                         breaks=c("O", "N", "Q", "V", "W"),
                         labels = c("Independent Expenditure-Only (Super PACs)", "PAC - Nonqualified", "PAC - Qualified", "PAC with Non-contribution Account - Nonqualified", "PAC with Non-contribution Account - Qualified")) +
  scale_colour_manual(name="PAC Type",
                      breaks=c("N", "O", "Q", "V", "W"), 
                      values = c((brewer.pal(5,"Set1"))),
                      labels = c("Independent Expenditure-Only (Super PACs)", "PAC - Nonqualified", "PAC - Qualified", "PAC with Non-contribution Account - Nonqualified", "PAC with Non-contribution Account - Qualified")) +
  geom_jitter(show.legend = T) +
  xlab("Log(Total Contributions)") +
  ylab("State of Registration") +
  ggtitle("Contributions of Committees by Registered State and PAC Type") +
  ggthemes::theme_tufte()

contb
```

```{r, fig.width=10, fig.height=6, echo = FALSE, warning=FALSE}
#Mapped STATE vs. expenditures
disb <- train_data2 %>% 
  ggplot(aes(TTL_DISB, CMTE_ST, color = factor(committee_type))) +
    scale_fill_discrete(name="PAC Type",
                         breaks=c("O", "N", "Q", "V", "W"),
                         labels = c("Independent Expenditure-Only (Super PACs)", "PAC - Nonqualified", "PAC - Qualified", "PAC with Non-contribution Account - Nonqualified", "PAC with Non-contribution Account - Qualified")) +
  scale_colour_manual(name="PAC Type",
                      breaks=c("N", "O", "Q", "V", "W"), 
                      values = c((brewer.pal(5,"Set1"))),
                      labels = c("Independent Expenditure-Only (Super PACs)", "PAC - Nonqualified", "PAC - Qualified", "PAC with Non-contribution Account - Nonqualified", "PAC with non-contribution account - Qualified")) +
  geom_jitter(show.legend = T) +
  xlab("Log(Total Disbursements)") +
  ylab("State of Registration") +
  ggtitle("Disbursements of Committees by Registered State and PAC Type") +
  ggthemes::theme_tufte()

disb
```

```{r, include = FALSE}
#Mapped STATE vs. expenditures
receipt <- train_data2 %>% 
  ggplot(aes(TTL_RECEIPTS, CMTE_ST, color = factor(committee_type))) +
    scale_fill_discrete(name="PAC Type",
                         breaks=c("O", "N", "Q", "V", "W"),
                         labels = c("Independent Expenditure-Only (Super PACs)", "PAC - Nonqualified", "PAC - Qualified", "PAC with Non-contribution Account - Nonqualified", "PAC with Non-contribution Account - Qualified")) +
  scale_colour_manual(name="PAC Type",
                      breaks=c("N", "O", "Q", "V", "W"), 
                      values = c((brewer.pal(5,"Set1"))),
                      labels = c("Independent Expenditure-Only (Super PACs)", "PAC - Nonqualified", "PAC - Qualified", "PAC with Non-contribution Account - Nonqualified", "PAC with Non-contribution Account - Qualified")) +
  geom_jitter(show.legend = T) +
  xlab("Log(Total Receipt)") +
  ylab("State of Registration") +
  ggtitle("Receipts of Committees by Registered State and PAC Type") +
  ggthemes::theme_tufte()

receipt

```

The second step in my pre-processing process was to turn some variables, like *party* and *ORG_TP*, from characters into factors. That way, the models I run will know that those are columns with pre-determined values instead of random character strings. Third, I created a recipe which did the following: 

- Impute all monetary variables, taking the 5 closest observations and averaging them out 
- Turn all categorical variables, like *CMTE_ST* and *party*, and turning them into dummy variables
- Normalize all numeric values to be within a specific range

This recipe was applied to both the training and test dataset.

```{r, include = FALSE}
###### ----------------------------- Recoding and recipe ---------------------------------######

#Turn party into an actual factor
train_data2 <- train_data2 %>% 
  mutate(party = ifelse(party == "DEM" | party == "REP" , party, "NONE"),
         party = ifelse(is.na(party), "NONE", party))

#Turn ORG_TP into an actual factor
train_data2 <- train_data2 %>% 
    mutate(ORG_TP = ifelse(ORG_TP == "C" | ORG_TP == "T" | ORG_TP == "M", ORG_TP, "NONE"),
         ORG_TP = ifelse(is.na(ORG_TP), "NONE", ORG_TP))

# CONVERT ALL CHARACTERS TO FACTOR
train_data2 <- train_data2 %>% mutate_if(is.character, as.factor)

# CHECK
str(train_data2)

# CONVERT RESPONSE VARIABLE TO FACTOR AND LABEL
x <- train_data2 %>% 
  select(SuperPAC_yes, party, first_file_date, last_file_date, CMTE_DSGN, CMTE_FILING_FREQ, CMTE_ST, TTL_CONTB, TTL_RECEIPTS, TTL_DISB, ORG_TP) %>% 
  mutate(SuperPAC_yes = as.factor(SuperPAC_yes),
         SuperPAC_yes = recode(SuperPAC_yes, `0` = "no", `1` = "yes"))

# CREATE RECIPE
rcp <- 
  recipe(SuperPAC_yes ~  .,x) %>% 
  step_knnimpute(TTL_CONTB, TTL_RECEIPTS, TTL_DISB, neighbors = 5) %>% 
  step_dummy(all_nominal(),-all_outcomes()) %>% 
  step_range(all_numeric()) %>%  # Normalize scale
  prep()

train_data_processed <- bake(rcp,x)
test_data_processed <- bake(rcp,x)

skimr::skim(train_data_processed)
skimr::skim(train_data_processed)

```

### Cross-Validation

Cross-validation is needed in order to best train the following models. In order to reduce the error as much as possible, the models learn on seperate "chunks" of the training model and then averages the error between all the "chunks" (or folds). In this analysis, the training data will be split into 5 folds. 
```{r, include = FALSE}
## -------------------------------- Cross-Validation --------------------------------##

# set a seed for replication purposes 
set.seed(1988)

# Partition the data into 5 equal folds
folds <- createFolds(train_data_processed$SuperPAC_yes, k = 5) 

sapply(folds,length)

#Validation conditions
control_conditions <- 
  trainControl(method='cv', # K-fold cross validation
               summaryFunction = twoClassSummary, # classification problem
               classProbs = TRUE, # classification problem
               index = folds # The indices for our folds (so they are always the same)
  )

skimr::skim(train_data_processed)


```

# Results

## Models

For this analysis, I ran four basic models:

- Logit Model: This linear extension model can include categorical features and, unlike traditional linear models, can accomodate for non-linear shapes. 
- K-Nearest Neighbors Model: Uses the nearest of the datapoint for a prediction. *k* can be a tuning parameter. Model *mod_knn2* can be seen extending the *k* values to look for the best fit.
- Classification and Regression Tress (CART): Takes one feature and determines the cut-off point where the "leaf nodes" are the most pure. In other words, where the variance of *y* is the smallest. For this analysis, there was a second CART model ran with changes to the complexity feature (how far the tree can grow). 
- Random Forest: This model uses the "bagging" method. By creating trees from different subsets of the training data, the model can aggregate the predictions. 

```{r, include = FALSE}
## ------------------------------------- Models -------------------------------------##


#Logit Model
mod_logit <-
  train(SuperPAC_yes ~ ., # Equation
        data=train_data_processed, # Training data 
        method = "glm", # logit function
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )

mod_logit



#K-Nearest Neighbors Model

mod_knn <-
  train(SuperPAC_yes ~  ., # Equation (outcome and everything else)
        data=train_data_processed, # Training data 
        method = "knn", # K-Nearest Neighbors Algorithm
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )

plot(mod_knn)

#K-Nearest Neighbors Model 2

knn_tune = expand.grid(k = c(1,3,10,50,60,70,80,100))
knn_tune

mod_knn2 <-
  train(SuperPAC_yes ~  ., # Equation (outcome and everything else)
        data=train_data_processed, # Training data 
        method = "knn", # K-Nearest Neighbors Algorithm
        metric = "ROC", # area under the curve
        tuneGrid = knn_tune,
        trControl = control_conditions
  )

plot(mod_knn2)


#CART

mod_cart <-
  train(SuperPAC_yes ~  ., # Equation (outcome and everything else)
        data=train_data_processed, # Training data 
        method = "rpart", # Classification Tree
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )

plot(mod_cart)

#CART2

tune_cart2 <- expand.grid(cp = c(0.0001)) # Complexity Parameter (how "deep" our trees should grow)

mod_cart2 <-
  train(SuperPAC_yes ~ ., # Equation (outcome and everything else)
        data=train_data_processed, # Training data 
        method = "rpart", # Classification Tree
        metric = "ROC", # area under the curve
        tuneGrid = tune_cart2, # Tuning parameters
        trControl = control_conditions
  )

fancyRpartPlot(mod_cart$finalModel)

#Random Forest

mod_rf <-
  train(SuperPAC_yes ~ ., # Equation (outcome and everything else)
        data=train_data_processed, # Training data 
        method = "ranger", # random forest (ranger is much faster than rf)
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )

plot(mod_rf)

```

## Model Comparison

In terms of sensitivity (obtaining a true positive), then the 2nd K-Nearest Neighbors model would be the best. This model, however, fails when it comes to specificity (obtaining a true negative). By looking at both of these values, random forest is the best fitting model. 


```{r, fig.width=7, fig.height=3, echo = FALSE}
## ------------------------------------- Model Comparison  -------------------------------------##

mod_list <-
  list(
    knn1 = mod_knn,
    knn2 = mod_knn2,
    cart1 = mod_cart,
    cart2 = mod_cart2,
    rf = mod_rf,
    logit = mod_logit
  )

# Generate Plot to compare output. 
dotplot(resamples(mod_list))

```

## Predictive Performance

Running this model with the unseen, test data gives great results. When trying to predict "yes" values with *SuperPAC_yes* variable, the model had over 99% specificity and sensitivity with a 95% confidence level. 

```{r, include = FALSE}
## ------------------------------------- Unpacking the Outcome: Predictive Performance -------------------------------------##

pred <- predict(mod_rf,newdata = test_data_processed)

confusionMatrix(pred,test_data_processed$SuperPAC_yes,positive='yes')
```

## Variable Importance

Variable importance can show us the top variables that contributed to the model. In this case, when the committee first filed and what organization type they are are the two top predictors for the random forest model. 

```{r, fig.width=7, fig.height=3, echo = FALSE, warning=FALSE, message=FALSE}
## ------------------------------------- Unpacking the Outcome: Variable Importance -------------------------------------##
require(vip) # Install the package
pred_wrapper <- function(object, newdata) {
  # wrapper function so vip knows how to calculate a prediction
  predict(object, data=newdata,type="response")$predictions[,"yes"]
}

# Generate a variable importance plot
permute_imp_plot <- 
  vip::vip(mod_rf$finalModel,
           data = train_data_processed,
           target = train_data_processed$SuperPAC_yes,
           train = train_data_processed %>% select(-SuperPAC_yes),
           reference_class = "yes",
           method="permute",
           pred_wrapper = pred_wrapper)

permute_imp_plot

```

# Discussion

In my project proposal for this assignment, I addressed the following criteria for success:

1. I finish this project, without regrets/concerns/adjustments, by December 14th, 2019 at 9:00pm and
that I’m proud of the final submission
2. This project contains valuable information regarding federal PACs in 2019 that can be used as an
easily accessible resource for anyone not accustomed to the vocabulary of campaign finance. In other
words, anyone who doesn’t work in the campaign finance world could easily look at this final project
and understand what it’s saying and its importance to the upcoming election
3. The final product is a document that I can show my supervisor, who is familiar with campaign finance
data, and that he would easily be able to understand the importance of such a document
4. At the end of this project, I can say with a reasonable degree of certainty:
a. Which party during the 2019 filing year has a more negative sentiment in consideration with PACs
b. Which party a PAC is connected with given a reasonable number of important factors (i.e. expenditures,
contributions, state, etc.)

I did not achieve all of these goals. One goal that I did not achieve was to analyze committee names through tokenziation to see which party had more negative/positive sentiment associated with it. After looking at my data more closely, I found that a vast majority of the observations did not have a party associated with them. However, I do believe that I made campaign finance relatively accessible yet informative in this document. 

If given more time, I would investigate the date variables in more depth. I would like to round them to their nearest month/year and then see if there are more patterns that I didn't see by looking at it as a continuous variable. Also, I would think about adding data in from more election years to see how the financials of PACs vs. Super PACs have distinguished themselves over time. 





















